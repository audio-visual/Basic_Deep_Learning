{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，模型参数量越大，（或者数据集越大），LR就需要更小 \n",
    "\n",
    "【如果固定住feature extraction backbone只微调head的话，可以调大LR】\n",
    "\n",
    "（如果batch size可以开的很大，那么会具有更稳定的梯度估计，可以适当增大LR）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cav_mae\n",
    "\n",
    "|        |pretrain| Finetune| 备注  |\n",
    "|--------|--------|---------|--------|\n",
    "|Dataset| AS-2M   | AS-20k| |\n",
    "|Optimizer| Adam   | Adam | |\n",
    "|backbone LR| 1e-4 | 5e-5 | 根据论文Lwf中说，finetune的LR一般是train的0.1-0.02x|\n",
    "|LR decay start epoch| 10 （Multi-step decay at 1/3 and 2/3） | 5 | 数据集比较小的话，衰减的时间要提前，避免过拟合 |\n",
    "|LR decay rate| 0.5  |  0.5|  0.5是比较缓和的衰减，通常是0.1|\n",
    "|LR decay step| 5   |   5 | |\n",
    "|Epochs| 25 |15 | |\n",
    "|batch_size| 4x27 | 36 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cav-mae pretrain\n",
    "# 从第lrscheduler_start个epoch开始做learning rate的衰减，每次衰减lr=lr*gamma,间隔为lrscheduler_step\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(args.lrscheduler_start, 1000, args.lrscheduler_step)),gamma=args.lrscheduler_decay)\n",
    "# print('The learning rate scheduler starts at {:d} epoch with decay rate of {:.3f} every {:d} epoches'.format(args.lrscheduler_start, args.lrscheduler_decay, args.lrscheduler_step))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
