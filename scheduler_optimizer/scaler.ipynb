{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GradScaler 的作用:为了支持 混合精度训练（Mixed Precision Training）**\n",
    "\n",
    "> 用 FP16 直接 .backward() 时，可能会得到 NaN 梯度\n",
    "\n",
    "先把 loss 放大（scale），以避免梯度消失 `scaler.scale(loss).backward()`\n",
    "\n",
    "执行优化器步骤时进行缩放处理：`scaler.step(optimizer)`\n",
    "\n",
    "更新 scaler，根据训练是否稳定自动调整 scale 值：`scaler.update()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**example**\n",
    "```python\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():  # 用半精度前向\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "    # 反向传播用 scaler\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
