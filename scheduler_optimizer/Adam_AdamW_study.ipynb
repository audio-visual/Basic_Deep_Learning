{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通俗的理解，\n",
    "\n",
    "**Adam 对于参数下降的幅度取决于这个参数的方向【m，一阶矩（动量）】和这个方词上变化的剧烈程度【v, 二阶矩（方差）】**\n",
    "\n",
    "**可见，越是变化的剧烈（v越大），更新的步幅就越小**\n",
    "\n",
    "而且，会对每个参数分别维护自己的 m 和 v，进行独立更新\n",
    "\n",
    "细节则包括：移动平均（下面的公式1，2），参数修正(变成了\\hat{}，避免过小)等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "对于每一个参数 $θ_i$：\n",
    "$$m_i \\leftarrow \\beta_1 * m_i + (1 - \\beta_1) * g_i  $$\n",
    "\n",
    "$$v_i \\leftarrow \\beta_2 * v_i + (1 - \\beta_2) * (g_i)^2  $$\n",
    "$$ \\theta_i \\leftarrow \\theta_i - lr * \\hat{m_i} / (\\sqrt{\\hat{v_i}} + \\epsilon) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而Adam对于参数的正则化处理会有一定的问题（这儿没太看懂），所以才有了AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdamW 把正则项“从梯度里拿出来”，直接对参数施加惩罚：\n",
    "\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta - lr * (\\hat{m} / \\sqrt{\\hat{m}}) - lr * \\lambda * \\theta $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对了Learning rate的偏好\n",
    "\n",
    "AdamW的learning rate一般是 1.5–3× Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Optimizer\t| Backbone LR|\n",
    "|-----------|------------|\n",
    "|Adam |\t1e-4 |\n",
    "|AdamW\t|2e-4 or 3e-4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
